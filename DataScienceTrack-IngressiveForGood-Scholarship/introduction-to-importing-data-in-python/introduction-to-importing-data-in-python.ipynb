{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223713d1",
   "metadata": {},
   "source": [
    "# Exploring your working directory\n",
    "In order to import data into Python, you should first have an idea of what files are in your working directory.\n",
    "\n",
    "IPython, which is running on DataCamp's servers, has a bunch of cool commands, including its magic commands. For example, starting a line with ! gives you complete system shell access. This means that the IPython magic command ! ls will display the contents of your current directory. Your task is to use the IPython magic command ! ls to check out the contents of your current directory and answer the following question: which of the following files is in your working directory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39e6214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Possible Answers\\n\\nhuck_finn.txt\\n\\ntitanic.csv\\n\\nmoby_dick.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Possible Answers\n",
    "\n",
    "huck_finn.txt\n",
    "\n",
    "titanic.csv\n",
    "\n",
    "moby_dick.txt\"\"\"\n",
    "\n",
    "#Answer= moby_dick.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347bfdf0",
   "metadata": {},
   "source": [
    "# Importing entire text files\n",
    "In this exercise, you'll be working with the file moby_dick.txt. It is a text file that contains the opening sentences of Moby Dick, one of the great American novels! Here you'll get experience opening a text file, printing its contents to the shell and, finally, closing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file: file\n",
    "file = open('moby_dick.txt' ,mode= 'r') # mode = 'r' for read only\n",
    "\n",
    "# Print it\n",
    "print(file.read()) \n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)\n",
    "\n",
    "# Close file\n",
    "file.close()\n",
    "\n",
    "# Check whether file is closed\n",
    "print(file.closed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bfbde",
   "metadata": {},
   "source": [
    "# Importing text files line by line\n",
    "For large files, we may not want to print all of their content to the shell: you may wish to print only the first few lines. Enter the readline() method, which allows you to do this. When a file called file is open, you can print out the first line by executing file.readline(). If you execute the same command again, the second line will print, and so on.\n",
    "\n",
    "In the introductory video, Hugo also introduced the concept of a context manager. He showed that you can bind a variable file by using a context manager construct:\n",
    "\n",
    "with open('huck_finn.txt') as file:\n",
    "While still within this construct, the variable file will be bound to open('huck_finn.txt'); thus, to print the file to the shell, all the code you need to execute is:\n",
    "\n",
    "with open('huck_finn.txt') as file:\n",
    "    print(file.readline())\n",
    "You'll now use these tools to print the first few lines of moby_dick.txt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81943dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Open moby_dick.txt using the with context manager and the variable file.\n",
    "Print the first three lines of the file to the shell by using readline() three times within \n",
    "the context manager.\n",
    "\"\"\"\n",
    "# Read & print the first 3 lines\n",
    "with open('moby_dick.txt') as file:\n",
    "    print(file.readline())\n",
    "    print(file.readline())\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0355533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pop quiz: examples of flat files\\nYou're now well-versed in importing text files and you're about to become a wiz at importing flat files. But can you remember exactly what a flat file is? Test your knowledge by answering the following question: which of these file types below is NOT an example of a flat file?\\n\\nAnswer the question\\n50XP\\nPossible Answers\\n\\nA .csv file.\\n\\nA tab-delimited .txt.\\n\\nA relational database (e.g. PostgreSQL).\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Pop quiz: examples of flat files\n",
    "You're now well-versed in importing text files and you're about to become a wiz at importing flat files. But can you remember exactly what a flat file is? Test your knowledge by answering the following question: which of these file types below is NOT an example of a flat file?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "A .csv file.\n",
    "\n",
    "A tab-delimited .txt.\n",
    "\n",
    "A relational database (e.g. PostgreSQL).\"\"\"\n",
    "#answer= relational database (e.g. PostgreSQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec8ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pop quiz: what exactly are flat files?\\nWhich of the following statements about flat files is incorrect?\\n\\nAnswer the question\\n50XP\\nPossible Answers\\n\\nFlat files consist of rows and each row is called a record.\\n\\nFlat files consist of multiple tables with structured relationships between the tables.\\n\\nA record in a flat file is composed of fields or attributes, each of which contains at most one item of information.\\n\\nFlat files are pervasive in data science.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Pop quiz: what exactly are flat files?\n",
    "Which of the following statements about flat files is incorrect?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "Flat files consist of rows and each row is called a record.\n",
    "\n",
    "Flat files consist of multiple tables with structured relationships between the tables.\n",
    "\n",
    "A record in a flat file is composed of fields or attributes, each of which contains at most one item of information.\n",
    "\n",
    "Flat files are pervasive in data science.\"\"\"\n",
    "#answer=Flat files consist of multiple tables with structured relationships \n",
    "#between the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d238529",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why we like flat files and the Zen of Python\\nIn PythonLand, there are currently hundreds of Python Enhancement Proposals, commonly referred to as PEPs. PEP8, for example, is a standard style guide for Python, written by our sensei Guido van Rossum himself. It is the basis for how we here at DataCamp ask our instructors to style their code. Another one of my favorites is PEP20, commonly called the Zen of Python. Its abstract is as follows:\\n\\nLong time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, only 19 of which have been written down.\\n\\nIf you don't know what the acronym BDFL stands for, I suggest that you look here. You can print the Zen of Python in your shell by typing import this into it! You're going to do this now and the 5th aphorism (line) will say something of particular interest.\\n\\nThe question you need to answer is: what is the 5th aphorism of the Zen of Python?\\n\\nInstructions\\n50 XP\\nPossible Answers\\n\\nFlat is better than nested.\\n\\nFlat files are essential for data science.\\n\\nThe world is representable as a flat file.\\n\\nFlatness is in the eye of the beholder.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Why we like flat files and the Zen of Python\n",
    "In PythonLand, there are currently hundreds of Python Enhancement Proposals, commonly referred to as PEPs. PEP8, for example, is a standard style guide for Python, written by our sensei Guido van Rossum himself. It is the basis for how we here at DataCamp ask our instructors to style their code. Another one of my favorites is PEP20, commonly called the Zen of Python. Its abstract is as follows:\n",
    "\n",
    "Long time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, only 19 of which have been written down.\n",
    "\n",
    "If you don't know what the acronym BDFL stands for, I suggest that you look here. You can print the Zen of Python in your shell by typing import this into it! You're going to do this now and the 5th aphorism (line) will say something of particular interest.\n",
    "\n",
    "The question you need to answer is: what is the 5th aphorism of the Zen of Python?\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "Flat is better than nested.\n",
    "\n",
    "Flat files are essential for data science.\n",
    "\n",
    "The world is representable as a flat file.\n",
    "\n",
    "Flatness is in the eye of the beholder.\"\"\"\n",
    "#answer=Flat is better than nested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bc6b9",
   "metadata": {},
   "source": [
    "# Using NumPy to import flat files\n",
    "In this exercise, you're now going to load the MNIST digit recognition dataset using the numpy function loadtxt() and see just how easy it can be:\n",
    "\n",
    "The first argument will be the filename.\n",
    "The second will be the delimiter which, in this case, is a comma.\n",
    "You can find more information about the MNIST dataset here on the webpage of Yann LeCun, who is currently Director of AI Research at Facebook and Founding Director of the NYU Center for Data Science, among many other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import numpy as np\n",
    "\n",
    "# Assign filename to variable: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load file as array: digits\n",
    "digits = np.loadtxt(file, delimiter=',')\n",
    "\n",
    "# Print datatype of digits\n",
    "print(type(digits))\n",
    "\n",
    "# Select and reshape a row\n",
    "im = digits[21, 1:]\n",
    "im_sq = np.reshape(im, (28, 28))\n",
    "\n",
    "# Plot reshaped data (matplotlib.pyplot already loaded as plt)\n",
    "plt.imshow(im_sq, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974bdec",
   "metadata": {},
   "source": [
    "# Customizing your NumPy import\n",
    "What if there are rows, such as a header, that you don't want to import? What if your file has a delimiter other than a comma? What if you only wish to import particular columns?\n",
    "\n",
    "There are a number of arguments that np.loadtxt() takes that you'll find useful:\n",
    "\n",
    "delimiter changes the delimiter that loadtxt() is expecting.\n",
    "- You can use ',' for comma-delimited.\n",
    "- You can use '\\t' for tab-delimited.\n",
    "- skiprows allows you to specify how many rows (not indices) you wish to skip\n",
    "usecols takes a list of the indices of the columns you wish to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d980ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1 , usecols={0,2})\n",
    "\n",
    "# Print data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba190c",
   "metadata": {},
   "source": [
    "# Importing different datatypes\n",
    "The file seaslug.txt\n",
    "\n",
    "- has a text header, consisting of strings\n",
    "- is tab-delimited.\n",
    "These data consists of percentage of sea slug larvae that had metamorphosed in a given time period. \n",
    "Read more here.\n",
    "\n",
    "Due to the header, if you tried to import it as-is using np.loadtxt(), Python would throw you a ValueError and tell you that it could not convert string to float. There are two ways to deal with this: firstly, you can set the data type argument dtype equal to str (for string).\n",
    "\n",
    "Alternatively, you can skip the first row as we have seen before, using the skiprows argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'seaslug.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "\n",
    "# Print the first element of data\n",
    "print(data[0])\n",
    "\n",
    "# Import data as floats and skip the first row: data_float\n",
    "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(data_float[9])\n",
    "\n",
    "# Plot a scatterplot of the data\n",
    "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('percentage of larvae')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4aa4e",
   "metadata": {},
   "source": [
    "# Working with mixed datatypes (1)\n",
    "Much of the time you will need to import datasets which have different datatypes in different columns; one column may contain strings and another floats, for example. The function np.loadtxt() will freak at this. There is another function, np.genfromtxt(), which can handle such structures. If we pass dtype=None to it, it will figure out what types each column should be.\n",
    "\n",
    "Import 'titanic.csv' using the function np.genfromtxt() as follows:\n",
    "\n",
    "data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)\n",
    "Here, the first argument is the filename, the second specifies the delimiter , and the third argument names tells us there is a header. Because the data are of different types, data is an object called a structured array. Because numpy arrays have to contain elements that are all the same type, the structured array solves this by being a 1D array, where each element of the array is a row of the flat file imported. You can test this by checking out the array's shape in the shell by executing np.shape(data).\n",
    "\n",
    "Accessing rows and columns of structured arrays is super-intuitive: to get the ith row, merely execute data[i] and to get the column with name 'Fare', execute data['Fare'].\n",
    "\n",
    "After importing the Titanic data as a structured array (as per the instructions above), print the entire column with the name Survived to the shell. What are the last 4 values of this column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4358c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Possible Answers\\n1,0,0,1.\\n1,2,0,0.\\n1,0,1,0.\\n0,1,1,1.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Possible Answers\n",
    "1,0,0,1.\n",
    "1,2,0,0.\n",
    "1,0,1,0.\n",
    "0,1,1,1.\n",
    "\"\"\"\n",
    "#answer=1010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f5146",
   "metadata": {},
   "source": [
    "# Working with mixed datatypes (2)\n",
    "You have just used np.genfromtxt() to import data containing mixed datatypes. There is also another function np.recfromcsv() that behaves similarly to np.genfromtxt(), except that its default dtype is None. In this exercise, you'll practice using this to achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949df7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'titanic.csv'\n",
    "\n",
    "# Import file using np.recfromcsv: d\n",
    "d= np.recfromcsv(file, delimiter = ',', names=True, dtype=None)\n",
    "\n",
    "# Print out first three entries of d\n",
    "print(d[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29609ab1",
   "metadata": {},
   "source": [
    "# Using pandas to import flat files as DataFrames (1)\n",
    "In the last exercise, you were able to import flat files containing columns with different datatypes as numpy arrays. However, the DataFrame object in pandas is a more appropriate structure in which to store such data and, thankfully, we can easily import files of mixed data types as DataFrames using the pandas functions read_csv() and read_table()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'titanic.csv'\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# View the head of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8259764",
   "metadata": {},
   "source": [
    "# Using pandas to import flat files as DataFrames (2)\n",
    "In the last exercise, you were able to import flat files into a pandas DataFrame. As a bonus, it is then straightforward to retrieve the corresponding numpy array using the attribute values. You'll now have a chance to do this using the MNIST dataset, which is available as digits.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame: data\n",
    "data = pd.read_csv(file, nrows=5 , header= None)\n",
    "\n",
    "# Build a numpy array from the DataFrame: data_array\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Print the datatype of data_array to the shell\n",
    "print(type(data_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf45d2",
   "metadata": {},
   "source": [
    "# Customizing your pandas import\n",
    "The pandas package is also great at dealing with many of the issues you will encounter when importing data as a data scientist, such as comments occurring in flat files, empty lines and missing values. Note that missing values are also commonly referred to as NA or NaN. To wrap up this chapter, you're now going to import a slightly corrupted copy of the Titanic dataset titanic_corrupt.txt, which\n",
    "\n",
    "- contains comments after the character '#'\n",
    "- is tab-delimited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e50c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep='\\t' , comment='#', na_values='Nothing')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c41ec3",
   "metadata": {},
   "source": [
    "\"\"\" Not so flat any more\n",
    "In Chapter 1, you learned how to use the IPython magic command ! ls to \n",
    "explore your current working directory. You can also do this natively in \n",
    "Python using the library os, which consists of miscellaneous operating \n",
    "system interfaces.\n",
    "The first line of the following code imports the library os, the second \n",
    "line stores the name of the current directory in a string called wd and \n",
    "the third outputs the contents of the directory in a list to the shell.\n",
    "\"\"\"\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)\n",
    "\n",
    "\"\"\"\n",
    "Run this code in the IPython shell and answer the following questions. \n",
    "Ignore the files that begin with ..\n",
    "Check out the contents of your current directory and answer the following \n",
    "questions: (1) which file is in your directory and NOT an example of a \n",
    "flat file; (2) why is it not a flat file?\n",
    "Instructions\n",
    "Possible Answers\n",
    "database.db is not a flat file because relational databases contain \n",
    "structured relationships and flat files do not.\n",
    "battledeath.xlsx is not a flat because it is a spreadsheet consisting of \n",
    "many sheets, not a single table.\n",
    "titanic.txt is not a flat file because it is a .txt, not a .csv.\n",
    "\"\"\"\n",
    "\n",
    "answer= battledeath.xlsx is not a flat because it is a spreadsheet consisting of \n",
    "many sheets, not a single table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a35d29",
   "metadata": {},
   "source": [
    "# Loading a pickled file\n",
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.\n",
    "\n",
    "In this exercise, you'll import the pickle package, open a previously pickled data structure from a file and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ea9be",
   "metadata": {},
   "source": [
    "# Listing sheets in Excel files\n",
    "Whether you like it or not, any working data scientist will need to deal with Excel spreadsheets at some point in time. You won't always want to do so in Excel, however!\n",
    "\n",
    "Here, you'll learn how to use pandas to import Excel spreadsheets and how to list the names of the sheets in any loaded .xlsx file.\n",
    "\n",
    "Recall from the video that, given an Excel file imported into a variable spreadsheet, you can retrieve a list of the sheet names using the attribute spreadsheet.sheet_names.\n",
    "\n",
    "Specifically, you'll be loading and checking out the spreadsheet 'battledeath.xlsx', modified from the Peace Research Institute Oslo's (PRIO) dataset. This data contains age-adjusted mortality rates due to war in various countries over several years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c79f1",
   "metadata": {},
   "source": [
    "# Importing sheets from Excel files\n",
    "In the previous exercises, you saw that the Excel file contains two sheets, '2002' and '2004'. The next step is to import these.\n",
    "\n",
    "In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca68f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xls.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf2884",
   "metadata": {},
   "source": [
    "# Customizing your spreadsheet import\n",
    "Here, you'll parse your spreadsheets and use additional arguments to skip rows, rename columns and select only particular columns.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "As before, you'll use the method parse(). This time, however, you'll add the additional arguments skiprows, names and usecols. These skip rows, name the columns and designate which columns to parse, respectively. All these arguments can be assigned to lists containing the specific row numbers, strings and column numbers, as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=[0], names=['Country','AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(1, usecols= [0] , skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "405541ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to import SAS7BDAT\\nHow do you correctly import the function SAS7BDAT() from the package sas7bdat?\\n\\nAnswer the question\\n50XP\\nPossible Answers\\n\\nimport SAS7BDAT from sas7bdat\\n\\n\\nfrom SAS7BDAT import sas7bdat\\n\\n\\nimport sas7bdat from SAS7BDAT\\n\\nfrom sas7bdat import SAS7BDAT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"How to import SAS7BDAT\n",
    "How do you correctly import the function SAS7BDAT() from the package sas7bdat?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "import SAS7BDAT from sas7bdat\n",
    "\n",
    "\n",
    "from SAS7BDAT import sas7bdat\n",
    "\n",
    "\n",
    "import sas7bdat from SAS7BDAT\n",
    "\n",
    "from sas7bdat import SAS7BDAT\"\"\"\n",
    "#answer=from sas7bdat import SAS7BDAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249413b",
   "metadata": {},
   "source": [
    "# Importing SAS files\n",
    "In this exercise, you'll figure out how to import a SAS file as a DataFrame using SAS7BDAT and pandas. The file 'sales.sas7bdat' is already in your working directory and both pandas and matplotlib.pyplot have already been imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "The data are adapted from the website of the undergraduate text book Principles of Econometrics by Hill, Griffiths and Lim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33877c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00a7f8",
   "metadata": {},
   "source": [
    "# Using read_stata to import Stata files\n",
    "The pandas package has been imported in the environment as pd and the file disarea.dta is in your working directory. The data consist of disease extents for several diseases in various countries (more information can be found here).\n",
    "\n",
    "What is the correct way of using the read_stata() function to import disarea.dta into the object df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d00c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Possible Answers\\n\\ndf = 'disarea.dta'\\n\\ndf = read_stata.pd('disarea.dta')\\n\\ndf = pd.read_stata('disarea.dta')\\n\\ndf = pd.read_stata(disarea.dta)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Possible Answers\n",
    "\n",
    "df = 'disarea.dta'\n",
    "\n",
    "df = read_stata.pd('disarea.dta')\n",
    "\n",
    "df = pd.read_stata('disarea.dta')\n",
    "\n",
    "df = pd.read_stata(disarea.dta)\"\"\"\n",
    "#answer=df = pd.read_stata('disarea.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370b601",
   "metadata": {},
   "source": [
    "# Importing Stata files\n",
    "Here, you'll gain expertise in importing Stata files as DataFrames using the pd.read_stata() function from pandas. The last exercise's file, 'disarea.dta', is still in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load Stata file into a pandas DataFrame: df\n",
    "df = pd.read_stata(\"disarea.dta\")\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "\n",
    "# Plot histogram of one column of the DataFrame\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extent of disease')\n",
    "plt.ylabel('Number of countries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5f306",
   "metadata": {},
   "source": [
    "# Using File to import HDF5 files\n",
    "The h5py package has been imported in the environment and the file LIGO_data.hdf5 is loaded in the object h5py_file.\n",
    "\n",
    "What is the correct way of using the h5py function, File(), to import the file in h5py_file into an object, h5py_data, for reading only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d617c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Possible Answers\\n\\nh5py_data = File(h5py_file, 'r')\\n\\nh5py_data = h5py.File(h5py_file, 'r')\\n\\nh5py_data = h5py.File(h5py_file, read)\\n\\nh5py_data = h5py.File(h5py_file, 'read')\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Possible Answers\n",
    "\n",
    "h5py_data = File(h5py_file, 'r')\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, 'r')\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, read)\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, 'read')\"\"\"\n",
    "#answer=h5py_data = h5py.File(h5py_file, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75015c5a",
   "metadata": {},
   "source": [
    "# Using h5py to import HDF5 files\n",
    "The file 'LIGO_data.hdf5' is already in your working directory. In this exercise, you'll import it using the h5py library. You'll also print out its datatype to confirm you have imported it correctly. You'll then study the structure of the file in order to see precisely what HDF groups it contains.\n",
    "\n",
    "You can find the LIGO data plus loads of documentation and tutorials here. There is also a great tutorial on Signal Processing with the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c98c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Assign filename: file\n",
    "file = \"LIGO_data.hdf5\"\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(file, \"r\")\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(data))\n",
    "\n",
    "# Print the keys of the file\n",
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7fedb",
   "metadata": {},
   "source": [
    "# Extracting data from your HDF5 file\n",
    "In this exercise, you'll extract some of the LIGO experiment's actual data from the HDF5 file and you'll visualize it.\n",
    "\n",
    "To do so, you'll need to first explore the HDF5 group 'strain'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de50776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HDF5 group: group\n",
    "group = data[\"strain\"]\n",
    "\n",
    "# Check out keys of group\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "strain = group['Strain'].value\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('strain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4a946",
   "metadata": {},
   "source": [
    "# Loading .mat files\n",
    "In this exercise, you'll figure out how to load a MATLAB file using scipy.io.loadmat() and you'll discover what Python datatype it yields.\n",
    "\n",
    "The file 'albeck_gene_expression.mat' is in your working directory. This file contains gene expression data from the Albeck Lab at UC Davis. You can find the data and some great documentation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5001a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "mat = scipy.io.loadmat(\"albeck_gene_expression.mat\")\n",
    "\n",
    "# Print the datatype type of mat\n",
    "print(type(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3b89d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The structure of .mat in Python\\nHere, you'll discover what is in the MATLAB dictionary that you loaded in the previous exercise.\\n\\nThe file 'albeck_gene_expression.mat' is already loaded into the variable mat. The following libraries have already been imported as follows:\\n\\nimport scipy.io\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nOnce again, this file contains gene expression data from the Albeck Lab at UCDavis. You can find the data and some great documentation here.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The structure of .mat in Python\n",
    "Here, you'll discover what is in the MATLAB dictionary that you loaded in the previous exercise.\n",
    "\n",
    "The file 'albeck_gene_expression.mat' is already loaded into the variable mat. The following libraries have already been imported as follows:\n",
    "\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Once again, this file contains gene expression data from the Albeck Lab at UCDavis. You can find the data and some great documentation here.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys of the MATLAB dictionary\n",
    "print(mat.keys())\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(np.shape(mat['CYratioCyt']))\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9538ff09",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pop quiz: The relational model\\nWhich of the following is not part of the relational model?\\n\\nAnswer the question\\n50XP\\nPossible Answers\\n\\nEach row or record in a table represents an instance of an entity type.\\n\\nEach column in a table represents an attribute or feature of an instance.\\n\\nEvery table contains a primary key column, which has a unique entry for each row.\\n\\nA database consists of at least 3 tables.\\n\\nThere are relations between tables.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Pop quiz: The relational model\n",
    "Which of the following is not part of the relational model?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "Each row or record in a table represents an instance of an entity type.\n",
    "\n",
    "Each column in a table represents an attribute or feature of an instance.\n",
    "\n",
    "Every table contains a primary key column, which has a unique entry for each row.\n",
    "\n",
    "A database consists of at least 3 tables.\n",
    "\n",
    "There are relations between tables.\"\"\"\n",
    "#answer=A database consists of at least 3 tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6daaf8",
   "metadata": {},
   "source": [
    "# Creating a database engine\n",
    "Here, you're going to fire up your very first SQL engine. You'll create an engine to connect to the SQLite database 'Chinook.sqlite', which is in your working directory. Remember that to create an engine to connect to 'Northwind.sqlite', Hugo executed the command\n",
    "\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "Here, 'sqlite:///Northwind.sqlite' is called the connection string to the SQLite database Northwind.sqlite. A little bit of background on the Chinook database: the Chinook database contains information about a semi-fictional digital media store in which media data is real and customer, employee and sales data has been manually created.\n",
    "\n",
    "Why the name Chinook, you ask? According to their website,\n",
    "\n",
    "The name of this sample database was based on the Northwind database. Chinooks are winds in the interior West of North America, where the Canadian Prairies and Great Plains meet various mountain ranges. Chinooks are most prevalent over southern Alberta in Canada. Chinook is a good name choice for a database that intends to be an alternative to Northwind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f830789",
   "metadata": {},
   "source": [
    "# What are the tables in the database?\n",
    "In this exercise, you'll once again create an engine to connect to 'Chinook.sqlite'. Before you can get any data out of the database, however, you'll need to know what tables it contains!\n",
    "\n",
    "To this end, you'll save the table names to a list using the method table_names() on the engine and then you will print the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d09f9e",
   "metadata": {},
   "source": [
    "The Hello World of SQL Queries!\n",
    "Now, it's time for liftoff! In this exercise, you'll perform the Hello World of SQL queries, SELECT, in order to retrieve all columns of the table Album in the Chinook database. Recall that the query SELECT * selects all columns.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "- Open the engine connection as con using the method connect() on the engine.\n",
    "- Execute the query that selects ALL columns from the Album table. Store the results in rs.\n",
    "- Store all of your query results in the DataFrame df by applying the fetchall() method to the results rs.\n",
    "- Close the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3059a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(\"SELECT * FROM Album\")\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577638df",
   "metadata": {},
   "source": [
    "Customizing the Hello World of SQL Queries\n",
    "Congratulations on executing your first SQL query! Now you're going to figure out how to customize your query in order to:\n",
    "\n",
    "Select specified columns from a table;\n",
    "Select a specified number of rows;\n",
    "Import column names from the database table.\n",
    "Recall that Hugo performed a very similar query customization in the video:\n",
    "\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5))\n",
    "    df.columns = rs.keys()\n",
    "Packages have already been imported as follows:\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "The engine has also already been created:\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "The engine connection is already open with the statement\n",
    "\n",
    "with engine.connect() as con:\n",
    "\n",
    "All the code you need to complete is within this context.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "- Execute the SQL query that selects the columns LastName and Title from the Employee table. Store the results in the variable rs.\n",
    "- Apply the method fetchmany() to rs in order to retrieve 3 of the records. - Store them in the DataFrame df.\n",
    "- Using the rs object, set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c369b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT LastName, Title FROM Employee\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=3))\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd96ff",
   "metadata": {},
   "source": [
    "# Filtering your database records using SQL's WHERE\n",
    "You can now execute a basic SQL query to select records from any table in your database and you can also perform simple query customizations to select particular columns and numbers of rows.\n",
    "\n",
    "There are a couple more standard SQL query chops that will aid you in your journey to becoming an SQL ninja.\n",
    "\n",
    "Let's say, for example that you wanted to get all records from the Customer table of the Chinook database for which the Country is 'Canada'. You can do this very easily in SQL using a SELECT statement followed by a WHERE clause as follows:\n",
    "\n",
    "SELECT * FROM Customer WHERE Country = 'Canada'\n",
    "In fact, you can filter any SELECT statement by any condition using a WHERE clause. This is called filtering your records.\n",
    "\n",
    "In this interactive exercise, you'll select all records of the Employee table for which 'EmployeeId' is greater than or equal to 6.\n",
    "\n",
    "Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "Query away!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "- Complete the argument of create_engine() so that the engine for the SQLite database 'Chinook.sqlite' is created.\n",
    "- Execute the query that selects all records from the Employee table where 'EmployeeId' is greater than or equal to 6. Use the >= operator and assign the results to rs.\n",
    "- Apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "- Using the rs object, set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee WHERE EmployeeId >=6\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1251c",
   "metadata": {},
   "source": [
    "# Ordering your SQL records with ORDER BY\n",
    "You can also order your SQL query results. For example, if you wanted to get all records from the Customer table of the Chinook database and order them in increasing order by the column SupportRepId, you could do so with the following query:\n",
    "\n",
    "\"SELECT * FROM Customer ORDER BY SupportRepId\"\n",
    "In fact, you can order any SELECT statement by any column.\n",
    "\n",
    "In this interactive exercise, you'll select all records of the Employee table and order them in increasing order by the column BirthDate.\n",
    "\n",
    "Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "Get querying!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "- Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "- In the context manager, execute the query that selects all records from the Employee table and orders them in increasing order by the column BirthDate. Assign the result to rs.\n",
    "- In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "- Set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecad266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbcdaf",
   "metadata": {},
   "source": [
    "# Pandas and The Hello World of SQL Queries!\n",
    "Here, you'll take advantage of the power of pandas to write the results of your SQL query to a DataFrame in one swift line of Python code!\n",
    "\n",
    "You'll first import pandas and create the SQLite 'Chinook.sqlite' engine. Then you'll query the database to select all records from the Album table.\n",
    "\n",
    "Recall that to select all records from the Orders table in the Northwind database, Hugo executed the following command:\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n",
    "Instructions\n",
    "100 XP\n",
    "- Import the pandas package using the alias pd.\n",
    "- Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "- Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from the table Album.\n",
    "- The remainder of the code is included to confirm that the DataFrame created by this method is equal to that created by the previous method that you learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3c789",
   "metadata": {},
   "source": [
    "# Pandas for more complex querying\n",
    "Here, you'll become more familiar with the pandas function read_sql_query() by using it to execute a more complex query: a SELECT statement followed by both a WHERE clause AND an ORDER BY clause.\n",
    "\n",
    "You'll build a DataFrame that contains the rows of the Employee table for which the EmployeeId is greater than or equal to 6 and you'll order these entries by BirthDate.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "- Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "- Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from the - Employee table where the EmployeeId is greater than or equal to 6 and ordered by BirthDate (make sure to use WHERE and ORDER BY in this precise order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43680b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Employee WHERE EmployeeId >=6 ORDER BY BirthDate\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4cf29",
   "metadata": {},
   "source": [
    "# The power of SQL lies in relationships between tables: INNER JOIN\n",
    "Here, you'll perform your first INNER JOIN! You'll be working with your favourite SQLite database, Chinook.sqlite. For each record in the Album table, you'll extract the Title along with the Name of the Artist. The latter will come from the Artist table and so you will need to INNER JOIN these two tables on the ArtistID column of both.\n",
    "\n",
    "Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:\n",
    "\n",
    "\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "\n",
    "The following code has already been executed to import the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "- Assign to rs the results from the following query: select all the records, extracting the Title of the record and Name of the artist of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN these two tables on the ArtistID column of both.\n",
    "- In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "- Set the DataFrame's column names to the corresponding names of the table columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02756d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistID = Artist.ArtistID\")\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee3ce7",
   "metadata": {},
   "source": [
    "# Filtering your INNER JOIN\n",
    "Congrats on performing your first INNER JOIN! You're now going to finish this chapter with one final exercise in which you perform an INNER JOIN and filter the result using a WHERE clause.\n",
    "\n",
    "Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:\n",
    "\n",
    "\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "The following code has already been executed to import the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "- Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId that satisfy the condition Milliseconds < 250000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7189e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId WHERE Milliseconds < 250000\",engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e330e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
