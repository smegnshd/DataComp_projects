{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised learning in real world\n",
    "# Which of the following examples can be solved with unsupervised learning?\n",
    "\n",
    "# Answer the question\n",
    "# 50 XP\n",
    "# Possible Answers\n",
    "# A list of tweets to be classified based on their sentiment, the data has tweets associated with a positive or negative sentiment.\n",
    "\n",
    "# A spam recognition system that marks incoming emails as spam, the data has emails marked as spam and not spam.\n",
    "\n",
    "# Segmentation of learners at DataCamp based on courses they complete. The training data has no labels.\n",
    "\n",
    "\"\"\"ANSWER\"\"\"\n",
    "\"\"\"PRESS 3\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f1c03",
   "metadata": {},
   "source": [
    "# Pokémon sightings\n",
    "There have been reports of sightings of rare, legendary Pokémon. You have been asked to investigate! Plot the coordinates of sightings to find out where the Pokémon might be. The X and Y coordinates of the points are stored in list x and y, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting class from matplotlib library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Display the scatter plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0eeea8",
   "metadata": {},
   "source": [
    "# Pokémon sightings: hierarchical clustering\n",
    "We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Remember that in the scatter plot of the previous exercise, you identified two areas where Pokémon sightings were dense. This means that the points seem to separate into two clusters. In this exercise, you will form two clusters of the sightings using hierarchical clustering.\n",
    "\n",
    "'x' and 'y' are columns of X and Y coordinates of the locations of sightings, stored in a Pandas data frame, df. The following are available for use: matplotlib.pyplot as plt, seaborn as sns, and pandas as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ec6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linkage and fcluster functions\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# Use the linkage() function to compute distances\n",
    "Z = linkage(df, 'ward')\n",
    "\n",
    "# Generate cluster labels\n",
    "df['cluster_labels'] = fcluster(Z, 2, criterion='maxclust')\n",
    "\n",
    "# Plot the points with seaborn\n",
    "sns.scatterplot(x='x', y='y', hue='cluster_labels', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063f21c",
   "metadata": {},
   "source": [
    "# Pokémon sightings: k-means clustering\n",
    "We are going to continue the investigation into the sightings of legendary Pokémon from the previous exercise. Just like the previous exercise, we will use the same example of Pokémon sightings. In this exercise, you will form clusters of the sightings using k-means clustering.\n",
    "\n",
    "x and y are columns of X and Y coordinates of the locations of sightings, stored in a Pandas data frame, df. The following are available for use: matplotlib.pyplot as plt, seaborn as sns, and pandas as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c11a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Compute cluster centers\n",
    "centroids,_ = kmeans(df, 2)\n",
    "\n",
    "# Assign cluster labels\n",
    "df['cluster_labels'], _ = vq(df, centroids)\n",
    "\n",
    "# Plot the points with seaborn\n",
    "sns.scatterplot(x='x', y='y', hue='cluster_labels', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6b053",
   "metadata": {},
   "source": [
    "# Normalize basic list data\n",
    "Now that you are aware of normalization, let us try to normalize some data. goals_for is a list of goals scored by a football team in their last ten matches. Let us standardize the data using the whiten() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da564e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import the whiten function.\n",
    "Use the whiten() function to standardize the data.\"\"\"\n",
    "# Import the whiten function\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "goals_for = [4,3,2,3,1,1,2,0,1,4]\n",
    "\n",
    "# Use the whiten() function to standardize the data\n",
    "scaled_data = whiten(goals_for)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56542095",
   "metadata": {},
   "source": [
    "# Visualize normalized data\n",
    "After normalizing your data, you can compare the scaled data to the original data to see the difference. The variables from the last exercise, goals_for and scaled_data are already available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34546f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original data\n",
    "plt.plot(goals_for, label='original')\n",
    "\n",
    "# Plot scaled data\n",
    "plt.plot(scaled_data, label='scaled')\n",
    "\n",
    "# Show the legend in the plot\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfea9c",
   "metadata": {},
   "source": [
    "# Normalization of small numbers\n",
    "In earlier examples, you have normalization of whole numbers. In this exercise, you will look at the treatment of fractional numbers - the change of interest rates in the country of Bangalla over the years. For your use, matplotlib.pyplot is imported as plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee11577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "rate_cuts = [0.0025, 0.001, -0.0005, -0.001, -0.0005, 0.0025, -0.001, -0.0015, -0.001, 0.0005]\n",
    "\n",
    "# Use the whiten() function to standardize the data\n",
    "scaled_data = whiten(rate_cuts)\n",
    "\n",
    "# Plot original data\n",
    "plt.plot(rate_cuts, label='original')\n",
    "\n",
    "# Plot scaled data\n",
    "plt.plot(scaled_data, label='scaled')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28763519",
   "metadata": {},
   "source": [
    "# FIFA 18: Normalize data\n",
    "FIFA 18 is a football video game that was released in 2017 for PC and consoles. The dataset that you are about to work on contains data on the 1000 top individual players in the game. You will explore various features of the data as we move ahead in the course. In this exercise, you will work with two columns, eur_wage, the wage of a player in Euros and eur_value, their current transfer market value.\n",
    "\n",
    "The data for this exercise is stored in a Pandas dataframe, fifa. whiten from scipy.cluster.vq and matplotlib.pyplot as plt have been pre-loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d35f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale wage and value\n",
    "fifa['scaled_wage'] = whiten(fifa['eur_wage'])\n",
    "fifa['scaled_value'] = whiten(fifa['eur_value'])\n",
    "\n",
    "\"\"\"Plot the scaled wages and transfer values of players using the .plot() method of Pandas.\"\"\"\n",
    "# Scale wage and value\n",
    "fifa['scaled_wage'] = whiten(fifa['eur_wage'])\n",
    "fifa['scaled_value'] = whiten(fifa['eur_value'])\n",
    "\n",
    "# Plot the two columns in a scatter plot\n",
    "fifa.plot(x='scaled_wage', y='scaled_value', kind='scatter')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"Check the mean and standard deviation of the scaled data using the .describe() method of Pandas.\n",
    "\"\"\"\n",
    "# Scale wage and value\n",
    "fifa['scaled_wage'] = whiten(fifa['eur_wage'])\n",
    "fifa['scaled_value'] = whiten(fifa['eur_value'])\n",
    "\n",
    "# Plot the two columns in a scatter plot\n",
    "fifa.plot(x='scaled_wage', y='scaled_value', kind = 'scatter')\n",
    "plt.show()\n",
    "\n",
    "# Check mean and standard deviation of scaled values\n",
    "print(fifa[['scaled_wage', 'scaled_value']].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4562a38",
   "metadata": {},
   "source": [
    "# Hierarchical clustering: ward method\n",
    "It is time for Comic-Con! Comic-Con is an annual comic-based convention held in major cities in the world. You have the data of last year's footfall, the number of people at the convention ground at a given time. You would like to decide the location of your stall to maximize sales. Using the ward method, apply hierarchical clustering to find the two points of attraction in the area.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'ward', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964d1ec",
   "metadata": {},
   "source": [
    "# Hierarchical clustering: single method\n",
    "Let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f27448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'single', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a483ed2",
   "metadata": {},
   "source": [
    "# Hierarchical clustering: complete method\n",
    "For the third and final time, let us use the same footfall dataset and check if any changes are seen if we use a different method for clustering.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the fcluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'complete', metric = 'euclidean')\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'] = fcluster(distance_matrix, 2, criterion='maxclust')\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab88a1",
   "metadata": {},
   "source": [
    "# Visualize clusters with matplotlib\n",
    "We have discussed that visualizations are necessary to assess the clusters that are formed and spot trends in your data. Let us now focus on visualizing the footfall dataset from Comic-Con using the matplotlib module.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time. cluster_labels has the cluster labels. A linkage object is stored in the variable distance_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyplot class\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Define a colors dictionary for clusters\n",
    "colors = {1:'red', 2:'blue'}\n",
    "\n",
    "# Plot a scatter plot\n",
    "comic_con.plot.scatter(x = 'x_scaled', \n",
    "                \t   y = 'y_scaled',\n",
    "                       c = comic_con['cluster_labels'].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789efc5d",
   "metadata": {},
   "source": [
    "# Visualize clusters with seaborn\n",
    "Let us now visualize the footfall dataset from Comic Con using the seaborn module. Visualizing clusters using seaborn is easier with the inbuild hue function for cluster labels.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time. cluster_labels has the cluster labels. A linkage object is stored in the variable distance_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the seaborn module\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot a scatter plot using seaborn\n",
    "sns.scatterplot(x='x_scaled', \n",
    "                y='y_scaled', \n",
    "                hue='cluster_labels', \n",
    "                data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85bbfa",
   "metadata": {},
   "source": [
    "# Create a dendrogram\n",
    "Dendrograms are branching diagrams that show the merging of clusters as we move through the distance matrix. Let us use the Comic Con footfall data to create a dendrogram.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time. cluster_labels has the cluster labels. A linkage object is stored in the variable distance_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dendrogram function\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(distance_matrix)\n",
    "\n",
    "# Display the dendogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many clusters in comic con data?\n",
    "# Given the dendrogram from the last exercise, how many clusters can you see in the data?\n",
    "\n",
    "# A dendrogram is stored in the variable dn. Use plt.show() to display the dendrogram.\n",
    "\n",
    "# Instructions\n",
    "# 50 XP\n",
    "# Possible Answers\n",
    "# 2 clusters\n",
    "# 3 clusters\n",
    "# 4 clusters\n",
    "\n",
    "\n",
    "\"\"\"Answers\n",
    "2 clusters\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5cda9",
   "metadata": {},
   "source": [
    "# Timing run of hierarchical clustering\n",
    "In earlier exercises of this chapter, you have used the data of Comic-Con footfall to create clusters. In this exercise you will time how long it takes to run the algorithm on DataCamp's system.\n",
    "\n",
    "Remember that you can time the execution of small code snippets with:\n",
    "\n",
    "%timeit sum([1, 3, 2])\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time. The timeit module and linkage function are already imported\n",
    "\n",
    "How long does it take to the run the linkage function on the comic con data?\n",
    "\"\"\"\n",
    " Possible Answers\n",
    "- 1-5 microseconds\n",
    "- 1-5 milliseconds\n",
    "- 1-5 seconds\n",
    "\n",
    "\"\"\"Answers\n",
    "1-5 milliseconds\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b373443",
   "metadata": {},
   "source": [
    "# FIFA 18: exploring defenders\n",
    "In the FIFA 18 dataset, various attributes of players are present. Two such attributes are:\n",
    "\n",
    "sliding tackle: a number between 0-99 which signifies how accurate a player is able to perform sliding tackles\n",
    "aggression: a number between 0-99 which signifies the commitment and will of a player\n",
    "These are typically high in defense-minded players. In this exercise, you will perform clustering based on these attributes in the data.\n",
    "\n",
    "This data consists of 5000 rows, and is considerably larger than earlier datasets. Running hierarchical clustering on this data can take up to 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fit the scaled data in columns scaled_sliding_tackle and scaled_aggression into a hierarchical clustering algorithm. Additionally, you may want to check how long it takes to run the data in the console using the timeit module.\"\"\"\n",
    "# Fit the data into a hierarchical clustering algorithm\n",
    "distance_matrix = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')\n",
    "\n",
    "\n",
    "\"\"\"Assign cluster labels to each row in the data using the fcluster() function (use 3 clusters).\"\"\"\n",
    "# Fit the data into a hierarchical clustering algorithm\n",
    "distance_matrix = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')\n",
    "\n",
    "# Assign cluster labels to each row of data\n",
    "fifa['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')\n",
    "\n",
    "\n",
    "\"\"\"Display cluster centers of each cluster with respect to the scaled columns by calculating the mean value for each cluster.\"\"\"\n",
    "# Fit the data into a hierarchical clustering algorithm\n",
    "distance_matrix = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')\n",
    "\n",
    "# Assign cluster labels to each row of data\n",
    "fifa['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')\n",
    "\n",
    "# Display cluster centers of each cluster\n",
    "print(fifa[['scaled_sliding_tackle', 'scaled_aggression', 'cluster_labels']].groupby('cluster_labels').mean())\n",
    "\n",
    "\"\"\"Create a scatter plot using seaborn with the scaled sliding tackle attribute on the x-axis and the scaled aggression attribute on the y-axis. Assign a different color to each cluster.\"\"\"\n",
    "# Fit the data into a hierarchical clustering algorithm\n",
    "distance_matrix = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')\n",
    "\n",
    "# Assign cluster labels to each row of data\n",
    "fifa['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')\n",
    "\n",
    "# Display cluster centers of each cluster\n",
    "print(fifa[['scaled_sliding_tackle', 'scaled_aggression', 'cluster_labels']].groupby('cluster_labels').mean())\n",
    "\n",
    "# Create a scatter plot through seaborn\n",
    "sns.scatterplot(x='scaled_sliding_tackle', y='scaled_aggression', hue='cluster_labels', data=fifa)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf8652",
   "metadata": {},
   "source": [
    "# K-means clustering: first exercise\n",
    "This exercise will familiarize you with the usage of k-means clustering on a dataset. Let us use the Comic Con dataset and check how k-means clustering works on it.\n",
    "\n",
    "Recall the two steps of k-means clustering:\n",
    "\n",
    "- Define cluster centers through kmeans() function. It has two required arguments: observations and number of clusters.\n",
    "- Assign cluster labels through the vq() function. It has two required arguments: observations and cluster centers.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n",
    "\n",
    "# Assign cluster labels\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9209e",
   "metadata": {},
   "source": [
    "# Runtime of k-means clustering\n",
    "Recall that it took a significantly long time to run hierarchical clustering. How long does it take to run the kmeans() function on the FIFA dataset?\n",
    "\n",
    "The data is stored in a Pandas data frame, fifa. scaled_sliding_tackle and scaled_aggression are the relevant scaled columns. timeit and kmeans have been imported.\n",
    "\n",
    "Cluster centers are defined through the kmeans() function. It has two required arguments: observations and number of clusters. You can use %timeit before a piece of code to check how long it takes to run. You can time the kmeans() function for three clusters on the fifa dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Answers\n",
    "# ~ 50 μs (microseconds)\n",
    "# ~ 50 ms (milliseconds)\n",
    "# ~ 50 s (seconds)\n",
    "\n",
    "\"\"\"Answers\n",
    " ~ 50 ms (milliseconds)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16113391",
   "metadata": {},
   "source": [
    "# Elbow method on distinct clusters\n",
    "Let us use the comic con data set to see how the elbow plot looks on a data set with distinct, well-defined clusters. You may want to display the data points before proceeding with the exercise.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists - num_clusters, distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "# Creat a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"==================MCQ====================\"\"\"\n",
    "# Question\n",
    "# From the elbow plot, how many clusters are there in the data?\n",
    "\n",
    "# Possible Answers\n",
    "# 2 clusters\n",
    "# 4 clusters\n",
    "# 6 clusters\n",
    "\n",
    "\"\"\"Answers\n",
    " 2 clusters\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810bdab",
   "metadata": {},
   "source": [
    "# Elbow method on uniform data\n",
    "In the earlier exercise, you constructed an elbow plot on data with well-defined clusters. Let us now see how the elbow plot looks on a data set with uniformly distributed points. You may want to display the data points on the console before proceeding with the exercise.\n",
    "\n",
    "The data is stored in a Pandas data frame, uniform_data. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "num_clusters = range(2, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(uniform_data[['x_scaled', 'y_scaled']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists - number of clusters and distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "# Creat a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"==================MCQ====================\"\"\"\n",
    "# Question\n",
    "# From the elbow plot, how many clusters are there in the data?\n",
    "\n",
    "# Possible Answers\n",
    "# Can not be determined\n",
    "# 3 clusters\n",
    "# 4 clusters\n",
    "\n",
    "\"\"\"Answers\n",
    " Can not be determined\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bcd10",
   "metadata": {},
   "source": [
    "# Impact of seeds on distinct clusters\n",
    "You noticed the impact of seeds on a dataset that did not have well-defined groups of clusters. In this exercise, you will explore whether seeds impact the clusters in the Comic Con data, where the clusters are well-defined.\n",
    "\n",
    "The data is stored in a Pandas data frame, comic_con. x_scaled and y_scaled are the column names of the standardized X and Y coordinates of people at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random class\n",
    "from numpy import random\n",
    "\n",
    "# Initialize seed\n",
    "random.seed(0)\n",
    "\n",
    "# Run kmeans clustering\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot the scatterplot\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Change your code from the earlier step so that the seed is initialized with a list [1, 2, 1000].\n",
    "\"\"\"\n",
    "# Import random class\n",
    "from numpy import random\n",
    "\n",
    "# Initialize seed\n",
    "random.seed([1, 2, 1000])\n",
    "\n",
    "# Run kmeans clustering\n",
    "cluster_centers, distortion = kmeans(comic_con[['x_scaled', 'y_scaled']], 2)\n",
    "comic_con['cluster_labels'], distortion_list = vq(comic_con[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot the scatterplot\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = comic_con)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f261652",
   "metadata": {},
   "source": [
    "# Uniform clustering patterns\n",
    "Now that you are familiar with the impact of seeds, let us look at the bias in k-means clustering towards the formation of uniform clusters.\n",
    "\n",
    "Let us use a mouse-like dataset for our next exercise. A mouse-like dataset is a group of points that resemble the head of a mouse: it has three clusters of points arranged in circles, one each for the face and two ears of a mouse.\n",
    "\n",
    "Here is how a typical mouse-like dataset looks like (Source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d92c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kmeans and vq functions\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "# Generate cluster centers\n",
    "cluster_centers, distortion = kmeans(mouse[['x_scaled', 'y_scaled']], 3)\n",
    "\n",
    "# Assign cluster labels\n",
    "mouse['cluster_labels'], distortion_list = vq(mouse[['x_scaled', 'y_scaled']], cluster_centers)\n",
    "\n",
    "# Plot clusters\n",
    "sns.scatterplot(x='x_scaled', y='y_scaled', \n",
    "                hue='cluster_labels', data = mouse)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3f90a",
   "metadata": {},
   "source": [
    "# FIFA 18: defenders revisited\n",
    "In the FIFA 18 dataset, various attributes of players are present. Two such attributes are:\n",
    "\n",
    "- defending: a number which signifies the defending attributes of a player\n",
    "- physical: a number which signifies the physical attributes of a player\n",
    "These are typically defense-minded players. In this exercise, you will perform clustering based on these attributes in the data.\n",
    "\n",
    "The following modules have been pre-loaded: kmeans, vq from scipy.cluster.vq, matplotlib.pyplot as plt, seaborn as sns. The data for this exercise is stored in a Pandas dataframe, fifa. The scaled variables are scaled_def and scaled_phy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c63a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a random seed in numpy\n",
    "random.seed([1000,2000])\n",
    "\n",
    "\"\"\"Fit the scaled data in columns scaled_def and scaled_phy into a k-means clustering algorithm with 3 clusters and assign cluster labels.\"\"\"\n",
    "# Set up a random seed in numpy\n",
    "random.seed([1000,2000])\n",
    "\n",
    "# Fit the data into a k-means algorithm\n",
    "cluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)\n",
    "\n",
    "# Assign cluster labels\n",
    "fifa['cluster_labels'],_ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)\n",
    "\n",
    "\"\"\"Display cluster centers of each cluster with respect to the scaled columns by calculating the mean value for each cluster.\"\"\"\n",
    "# Set up a random seed in numpy\n",
    "random.seed([1000,2000])\n",
    "\n",
    "# Fit the data into a k-means algorithm\n",
    "cluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)\n",
    "\n",
    "# Assign cluster labels\n",
    "fifa['cluster_labels'], _ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)\n",
    "\n",
    "# Display cluster centers \n",
    "print(fifa[['scaled_def', 'scaled_phy', 'cluster_labels']].groupby('cluster_labels').mean())\n",
    "\n",
    "\"\"\"Create a seaborn scatter plot with scaled_def on the x-axis and scaled_phy on the y-axis, with each cluster represented by a different color.\n",
    "\"\"\"\n",
    "# Set up a random seed in numpy\n",
    "random.seed([1000,2000])\n",
    "\n",
    "# Fit the data into a k-means algorithm\n",
    "cluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)\n",
    "\n",
    "# Assign cluster labels\n",
    "fifa['cluster_labels'], _ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)\n",
    "\n",
    "# Display cluster centers \n",
    "print(fifa[['scaled_def', 'scaled_phy', 'cluster_labels']].groupby('cluster_labels').mean())\n",
    "\n",
    "# Create a scatter plot through seaborn\n",
    "sns.scatterplot(x='scaled_def', y='scaled_phy', hue='cluster_labels', data=fifa)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e28c2",
   "metadata": {},
   "source": [
    "# Extract RGB values from image\n",
    "There are broadly three steps to find the dominant colors in an image:\n",
    "\n",
    "- Extract RGB values into three lists.\n",
    "- Perform k-means clustering on scaled RGB values.\n",
    "- Display the colors of cluster centers.\n",
    "\n",
    "To extract RGB values, we use the imread() function of the image class of matplotlib. Empty lists, r, g and b have been initialized.\n",
    "\n",
    "For the purpose of finding dominant colors, we will be using the following image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44370a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image class of matplotlib\n",
    "import matplotlib.image as img\n",
    "\n",
    "# Read batman image and print dimensions\n",
    "batman_image = img.imread('batman.jpg')\n",
    "print(batman_image.shape)\n",
    "\n",
    "# Store RGB values of all pixels in lists r, g and b\n",
    "for row in batman_image:\n",
    "    for temp_r, temp_g, temp_b in row:\n",
    "        r.append(temp_r)\n",
    "        g.append(temp_g)\n",
    "        b.append(temp_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392413e6",
   "metadata": {},
   "source": [
    "# How many dominant colors?\n",
    "We have loaded the following image using the imread() function of the image class of matplotlib.\n",
    "\n",
    "\n",
    "\n",
    "The RGB values are stored in a data frame, batman_df. The RGB values have been standardized used the whiten() function, stored in columns, scaled_red, scaled_blue and scaled_green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "\n",
    "# Create a list of distortions from the kmeans function\n",
    "for i in num_clusters:\n",
    "    cluster_centers, distortion = kmeans(batman_df[['scaled_red', 'scaled_blue', 'scaled_green']], i)\n",
    "    distortions.append(distortion)\n",
    "\n",
    "# Create a data frame with two lists, num_clusters and distortions\n",
    "elbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n",
    "\n",
    "# Create a line plot of num_clusters and distortions\n",
    "sns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\n",
    "plt.xticks(num_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c0f47",
   "metadata": {},
   "source": [
    "# Display dominant colors\n",
    "We have loaded the following image using the imread() function of the image class of matplotlib.\n",
    "\n",
    "\n",
    "\n",
    "To display the dominant colors, convert the colors of the cluster centers to their raw values and then converted them to the range of 0-1, using the following formula: converted_pixel = standardized_pixel * pixel_std / 255\n",
    "\n",
    "The RGB values are stored in a data frame, batman_df. The scaled RGB values are stored in columns, scaled_red, scaled_blue and scaled_green. The cluster centers are stored in the variable cluster_centers, which were generated using the kmeans() function with three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard deviations of each color\n",
    "r_std, g_std, b_std = batman_df[['red', 'green', 'blue']].std()\n",
    "\n",
    "for cluster_center in cluster_centers:\n",
    "    scaled_r, scaled_g, scaled_b = cluster_center\n",
    "    # Convert each standardized value to scaled value\n",
    "    colors.append((\n",
    "        scaled_r * r_std / 255,\n",
    "        scaled_g * g_std / 255,\n",
    "        scaled_b * b_std / 255\n",
    "    ))\n",
    "\n",
    "# Display colors of cluster centers\n",
    "plt.imshow([colors])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad29e927",
   "metadata": {},
   "source": [
    "# TF-IDF of movie plots\n",
    "Let us use the plots of randomly selected movies to perform document clustering on. Before performing clustering on documents, they need to be cleaned of any unwanted noise (such as special characters and stop words) and converted into a sparse matrix through TF-IDF of the documents.\n",
    "\n",
    "Use the TfidfVectorizer class to perform the TF-IDF of movie plots stored in the list plots. The remove_noise() function is available to use as a tokenizer in the TfidfVectorizer class. The .fit_transform() method fits the data into the TfidfVectorizer objects and then generates the TF-IDF sparse matrix.\n",
    "\n",
    "Note: It takes a few seconds to run the .fit_transform() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeeb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer class from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, max_features=50,\n",
    "                                   min_df=0.1, tokenizer=remove_noise)\n",
    "\n",
    "# Use the .fit_transform() method on the list plots\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc63b68",
   "metadata": {},
   "source": [
    "# Top terms in movie clusters\n",
    "Now that you have created a sparse matrix, generate cluster centers and print the top three terms in each cluster. Use the .todense() method to convert the sparse matrix, tfidf_matrix to a normal matrix for the kmeans() function to process. Then, use the .get_feature_names() method to get a list of terms in the tfidf_vectorizer object. The zip() function in Python joins two lists.\n",
    "\n",
    "The tfidf_vectorizer object and sparse matrix, tfidf_matrix, from the previous have been retained in this exercise. kmeans has been imported from SciPy.\n",
    "\n",
    "With a higher number of data points, the clusters formed would be defined more clearly. However, this requires some computational power, making it difficult to accomplish in an exercise here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e452c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "\n",
    "# Generate cluster centers through the kmeans function\n",
    "cluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n",
    "\n",
    "# Generate terms from the tfidf_vectorizer object\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    # Sort the terms and print top 3 terms\n",
    "    center_terms = dict(zip(terms, list(cluster_centers[i])))\n",
    "    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n",
    "    print(sorted_terms[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with many features\n",
    "# What should you do if you have too many features for clustering?\n",
    "\n",
    "# Answer the question\n",
    "# 50 XP\n",
    "# Possible Answers\n",
    "\n",
    "# Visualize all the features\n",
    "\n",
    "# Reduce features using a technique like Factor Analysis\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "\n",
    "\"\"\"\n",
    "Answers\n",
    "Reduce features using a technique like Factor Analysis\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1af780",
   "metadata": {},
   "source": [
    "# Basic checks on clusters\n",
    "In the FIFA 18 dataset, we have concentrated on defenders in previous exercises. Let us try to focus on attacking attributes of a player. Pace (pac), Dribbling (dri) and Shooting (sho) are features that are present in attack minded players. In this exercise, k-means clustering has already been applied on the data using the scaled values of these three attributes. Try some basic checks on the clusters so formed.\n",
    "\n",
    "The data is stored in a Pandas data frame, fifa. The scaled column names are present in a list scaled_features. The cluster labels are stored in the cluster_labels column. Recall the .count() and .mean() methods in Pandas help you find the number of observations and mean of observations in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the size of the clusters\n",
    "print(fifa.groupby('cluster_labels')['ID'].count())\n",
    "\n",
    "# Print the mean value of wages in each cluster\n",
    "print(fifa.groupby('cluster_labels')['eur_wage'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c8d04",
   "metadata": {},
   "source": [
    "# FIFA 18: what makes a complete player?\n",
    "The overall level of a player in FIFA 18 is defined by six characteristics: pace (pac), shooting (sho), passing (pas), dribbling (dri), defending (def), physical (phy).\n",
    "\n",
    "Here is a sample card:\n",
    "\n",
    "Eden Hazard Player Card\n",
    "\n",
    "In this exercise, you will use all six characteristics to create clusters. The data for this exercise is stored in a Pandas dataframe, fifa. features is the list of these column names and scaled_features is the list of columns which contains their scaled values. The following have been pre-loaded: kmeans, vq from scipy.cluster.vq, matplotlib.pyplot as plt, seaborn as sns.\n",
    "\n",
    "Before you start the exercise, you may wish to explore scaled_features in the console to check out the list of six scaled columns names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f129a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use the kmeans() algorithm to create 2 clusters using the scaled features.\"\"\"\n",
    "# Create centroids with kmeans for 2 clusters\n",
    "cluster_centers,_ = kmeans(fifa[scaled_features], 2)\n",
    "\n",
    "\"\"\"Assign cluster labels to each row and print cluster centers using the .mean() method of Pandas.\"\"\"\n",
    "# Create centroids with kmeans for 2 clusters\n",
    "cluster_centers,_ = kmeans(fifa[scaled_features], 2)\n",
    "\n",
    "# Assign cluster labels and print cluster centers\n",
    "fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)\n",
    "print(fifa.groupby('cluster_labels')[scaled_features].mean())\n",
    "\n",
    "\"\"\"Plot a bar chart of scaled attributes of each cluster center using the .plot() method of Pandas.\"\"\"\n",
    "# Create centroids with kmeans for 2 clusters\n",
    "cluster_centers,_ = kmeans(fifa[scaled_features], 2)\n",
    "\n",
    "# Assign cluster labels and print cluster centers\n",
    "fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)\n",
    "print(fifa.groupby('cluster_labels')[scaled_features].mean())\n",
    "\n",
    "# Plot cluster centers to visualize clusters\n",
    "fifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind='bar')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Print the names of top 5 players in each cluster, using the name column.\"\"\"\n",
    "# Create centroids with kmeans for 2 clusters\n",
    "cluster_centers,_ = kmeans(fifa[scaled_features], 2)\n",
    "\n",
    "# Assign cluster labels and print cluster centers\n",
    "fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)\n",
    "print(fifa.groupby('cluster_labels')[scaled_features].mean())\n",
    "\n",
    "# Plot cluster centers to visualize clusters\n",
    "fifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# Get the name column of top 5 players in each cluster\n",
    "for cluster in fifa['cluster_labels'].unique():\n",
    "    print(cluster, fifa[fifa['cluster_labels'] == cluster]['name'].values[:5])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
