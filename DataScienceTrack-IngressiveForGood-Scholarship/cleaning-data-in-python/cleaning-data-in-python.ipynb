{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1fd9cf",
   "metadata": {},
   "source": [
    "# Numeric data or ... ?\n",
    "In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called ride_sharing. It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.\n",
    "\n",
    "The user_type column contains information on whether a user is taking a free ride and takes on the following values:\n",
    "\n",
    "1 for free riders.\n",
    "\n",
    "2 for pay per ride.\n",
    "\n",
    "3 for monthly subscribers.\n",
    "\n",
    "In this instance, you will print the information of ride_sharing using .info() and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset. The pandas package is imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 # Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc2d99",
   "metadata": {},
   "source": [
    "2\n",
    "Question\n",
    "By looking at the summary statistics - they don't really seem to offer much description on how users are distributed along their purchase type, why do you think that is?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- The user_type column is not of the correct type, it should be converted to str.\n",
    "\n",
    "- The user_type column has an infinite set of possible values, it should be converted to category.\n",
    "\n",
    "- The user_type column has an finite set of possible values that represent groupings of data, it should be converted to category.\n",
    "\n",
    "\"\"\"answer=The user_type column has an finite set of possible values that represent groupings of data, it should be converted to category\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b74b20",
   "metadata": {},
   "source": [
    "# Summing strings and concatenating numbers\n",
    "In the previous exercise, you were able to identify that category is the correct data type for user_type and convert it in order to extract relevant statistical summaries that shed light on the distribution of user_type.\n",
    "\n",
    "Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.\n",
    "\n",
    "In this exercise, you'll be converting the string column duration to the type int. Before that however, you will need to make sure to strip \"minutes\" from the column in order to make sure pandas reads it as numerical. The pandas package has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ffed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].sum()/ride_sharing['duration_time'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39e5f0",
   "metadata": {},
   "source": [
    "# Tire size constraints\n",
    "In this lesson, you're going to build on top of the work you've been doing with the ride_sharing DataFrame. You'll be working with the tire_sizes column which contains data on each bike's tire size.\n",
    "\n",
    "Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.\n",
    "\n",
    "In this exercise, you will make sure the tire_sizes column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d449a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e974c6f",
   "metadata": {},
   "source": [
    "# Back to the future\n",
    "A new update to the data pipeline feeding into the ride_sharing DataFrame has been updated to register each ride's date. This information is stored in the ride_date column of the type object, which represents strings in pandas.\n",
    "\n",
    "A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the ride_date column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert ride_date to a datetime object.\n",
    "\n",
    "The datetime package has been imported as dt, alongside all the packages you've been using till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ride_date to datetime\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34381029",
   "metadata": {},
   "source": [
    "# How big is your subset?\n",
    "You have the following loans DataFrame which contains loan and credit score data for consumers, and some metadata such as their first and last names. You want to find both complete and incomplete duplicates using .duplicated().\n",
    "\n",
    "|first_name|\t last_name|\t credit_score|\t has_loan|\n",
    "| --- | --- | --- | ---|\n",
    "|Justin|\tSaddle| meyer|\t600|\t1|\n",
    "|Hadrien|\tLacroix|\t450|\t0|\n",
    "\n",
    "Choose the correct usage of .duplicated() below:\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "loans.duplicated()\n",
    "  Because the default method returns both complete and incomplete duplicates.\n",
    "\n",
    "\n",
    "loans.duplicated(subset = 'first_name')\n",
    "  Because constraining the duplicate rows to the first name lets me find incomplete duplicates as well.\n",
    "\n",
    "\n",
    "loans.duplicated(subset = ['first_name', 'last_name'], keep = False)\n",
    "  Because subsetting on consumer metadata and not discarding any duplicate returns all duplicated rows.\n",
    "\n",
    "\n",
    "loans.duplicated(subset = ['first_name', 'last_name'], keep = 'first')\n",
    "  Because this drops all duplicates.\n",
    "  \n",
    "  \"\"\"answer=loans.duplicated(subset = ['first_name', 'last_name'], keep = False)\n",
    "  Because subsetting on consumer metadata and not discarding any duplicate returns all duplicated rows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426e4ac",
   "metadata": {},
   "source": [
    "# Finding duplicates\n",
    "A new update to the data pipeline feeding into ride_sharing has added the ride_id column, which represents a unique identifier for each ride.\n",
    "\n",
    "The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the ride_sharing DataFrame.\n",
    "\n",
    "In this exercise, you will confirm this suspicion by finding those duplicates. A sample of ride_sharing is in your environment, as well as all the packages you've been working with thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfabd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset=['ride_id'], keep=False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values(by = 'ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16270120",
   "metadata": {},
   "source": [
    "# Treating duplicates\n",
    "In the last exercise, you were able to verify that the new update feeding into ride_sharing contains a bug generating both complete and incomplete duplicated rows for some values of the ride_id column, with occasional discrepant values for the user_birth_year and duration columns.\n",
    "\n",
    "In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average duration, and the minimum user_birth_year for each set of incomplete duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70dc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby(by = 'ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f385883",
   "metadata": {},
   "source": [
    "# Finding consistency\n",
    "In this exercise and throughout this chapter, you'll be working with the airlines DataFrame which contains survey responses on the San Francisco Airport from airline customers.\n",
    "\n",
    "The DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction. Another DataFrame named categories was created, containing all correct possible values for the survey columns.\n",
    "\n",
    "In this exercise, you will use both of these DataFrames to find survey answers with inconsistent values, and drop them, effectively performing an outer and inner join on both these DataFrames as seen in the video exercise. The pandas package has been imported as pd, and the airlines and categories DataFrames are in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d66c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6c900",
   "metadata": {},
   "source": [
    "# Question\n",
    "Take a look at the output. Out of the cleanliness, safety and satisfaction columns, which one has an inconsistent category and what is it?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- cleanliness because it has an Unacceptable category.\n",
    "\n",
    "- cleanliness because it has a Terribly dirty category.\n",
    "\n",
    "- satisfaction because it has a Very satisfied category.\n",
    "\n",
    "- safety because it has a Neutral category.\n",
    "\n",
    "\"\"\"answer=cleanliness because it has an Unacceptable category.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2193a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f739107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cdc80",
   "metadata": {},
   "source": [
    "# Inconsistent categories\n",
    "In this exercise, you'll be revisiting the airlines DataFrame from the previous lesson.\n",
    "\n",
    "As a reminder, the DataFrame contains flight metadata such as the airline, the destination, waiting times as well as answers to key questions regarding cleanliness, safety, and satisfaction on the San Francisco Airport.\n",
    "\n",
    "In this exercise, you will examine two categorical columns from this DataFrame, dest_region and dest_size respectively, assess how to address them and make sure that they are cleaned and ready for analysis. The pandas package has been imported as pd, and the airlines DataFrame is in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7e5a3",
   "metadata": {},
   "source": [
    "# Question\n",
    "From looking at the output, what do you think is the problem with these columns?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- The dest_region column has only inconsistent values due to capitalization.\n",
    "\n",
    "- The dest_region column has inconsistent values due to capitalization and has one value that needs to be remapped.\n",
    "\n",
    "- The dest_size column has only inconsistent values due to leading and trailing spaces.\n",
    "\n",
    "- Both 2 and 3 are correct.\n",
    "\n",
    "\"\"\"answer=- Both 2 and 3 are correct.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ff9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33304871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ade0a",
   "metadata": {},
   "source": [
    "# Remapping categories\n",
    "To better understand survey respondents from airlines, you want to find out if there is a relationship between certain responses and the day of the week and wait time at the gate.\n",
    "\n",
    "The airlines DataFrame contains the day and wait_min columns, which are categorical and numerical respectively. The day column contains the exact day a flight took place, and wait_min contains the amount of minutes it took travelers to wait at the gate. To make your analysis easier, you want to create two new categorical variables:\n",
    "\n",
    "- wait_type: 'short' for 0-60 min, 'medium' for 60-180 and long for 180+\n",
    "- day_week: 'weekday' if day is in the weekday, 'weekend' if day is in the weekend.\n",
    "The pandas and numpy packages have been imported as pd and np. Let's create some new categorical data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264e4f0",
   "metadata": {},
   "source": [
    "# Removing titles and taking names\n",
    "While collecting survey respondent metadata in the airlines DataFrame, the full name of respondents was saved in the full_name column. However upon closer inspection, you found that a lot of the different names are prefixed by honorifics such as \"Dr.\", \"Mr.\", \"Ms.\" and \"Miss\".\n",
    "\n",
    "Your ultimate objective is to create two new columns named first_name and last_name, containing the first and last names of respondents respectively. Before doing so however, you need to remove honorifics.\n",
    "\n",
    "The airlines DataFrame is in your environment, alongside pandas as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afd645",
   "metadata": {},
   "source": [
    "# Keeping it descriptive\n",
    "To further understand travelers' experiences in the San Francisco Airport, the quality assurance department sent out a qualitative questionnaire to all travelers who gave the airport the worst score on all possible categories. The objective behind this questionnaire is to identify common patterns in what travelers are saying about the airport.\n",
    "\n",
    "Their response is stored in the survey_response column. Upon a closer look, you realized a few of the answers gave the shortest possible character amount without much substance. In this exercise, you will isolate the responses with a character count higher than 40 , and make sure your new DataFrame contains responses with 40 characters or more using an assert statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9aa14",
   "metadata": {},
   "source": [
    "# Ambiguous dates\n",
    "You have a DataFrame containing a subscription_date column that was collected from various sources with different Date formats such as YYYY-mm-dd and YYYY-dd-mm. What is the best way to unify the formats for ambiguous values such as 2019-04-07?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "- Set them to NA and drop them.\n",
    "\n",
    "- Infer the format of the data in question by checking the format of subsequent and previous values.\n",
    "\n",
    "- Infer the format from the original data source.\n",
    "\n",
    "- All of the above are possible, as long as we investigate where our data comes from, and understand the dynamics affecting it before cleaning it.\n",
    "\"\"\"answer=All of the \"\"\"above are possible, as long as we investigate where our data comes from, and understand the dynamics affecting it before cleaning it.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b631e",
   "metadata": {},
   "source": [
    "# Uniform currencies\n",
    "In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the banking DataFrame. The dataset contains data on the amount of money stored in accounts (acct_amount), their currency (acct_cur), amount invested (inv_amount), account opening date (account_opened), and last transaction date (last_transaction) that were consolidated from American and European branches.\n",
    "\n",
    "You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars. The pandas package has been imported as pd, and the banking DataFrame is in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8dcb6",
   "metadata": {},
   "source": [
    "# Uniform dates\n",
    "After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The account_opened column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.\n",
    "\n",
    "However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a datetime object, while making sure that the format is inferred and potentially incorrect formats are set to missing. The banking DataFrame is in your environment and pandas was imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e3407",
   "metadata": {},
   "source": [
    "# Question\n",
    "Take a look at the output. You tried converting the values to datetime using the default to_datetime() function without changing any argument, however received the following error:\n",
    "\n",
    "ValueError: month must be in 1..12\n",
    "Why do you think that is?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- The to_datetime() function needs to be explicitly told which date format each row is in.\n",
    "\n",
    "- The to_datetime() function can only be applied on YY-mm-dd date formats.\n",
    "\n",
    "- The 21-14-17 entry is erroneous and leads to an error.\n",
    "\"\"\"answer=The 21-14-17 entry is erroneous and leads to an error.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22527ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Infer datetime format\n",
    "                                           infer_datetime_format = True,\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dfe173",
   "metadata": {},
   "source": [
    "# How's our data integrity?\n",
    "New data has been merged into the banking DataFrame that contains details on how investments in the inv_amount column are allocated across four different funds A, B, C and D.\n",
    "\n",
    "Furthermore, the age and birthdays of customers are now stored in the age and birth_date columns respectively.\n",
    "\n",
    "You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of inv_amount and age against the amount invested in different funds and customers' birthdays. Both pandas and datetime have been imported as pd and dt respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a181668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0813f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = ages_manual == banking['age']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81852c61",
   "metadata": {},
   "source": [
    "# Is this missing at random?\n",
    "You've seen in the video exercise how there are a variety of missingness types when observing missing data. As a reminder, missingness types can be described as the following:\n",
    "\n",
    "- Missing Completely at Random: No systematic relationship between a column's missing values and other or own values.\n",
    "- Missing at Random: There is a systematic relationship between a column's missing values and other observed values.\n",
    "- Missing not at Random: There is a systematic relationship between a column's missing values and unobserved values.\n",
    "You have a DataFrame containing customer satisfaction scores for a service. What type of missingness is the following?\n",
    "\n",
    "  A customer satisfaction_score column with missing values for highly dissatisfied customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0b0e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Possible Answers\\n\\nMissing completely at random.\\n\\nMissing at random.\\n\\nMissing not at random.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Possible Answers\n",
    "\n",
    "Missing completely at random.\n",
    "\n",
    "Missing at random.\n",
    "\n",
    "Missing not at random.\"\"\"\n",
    "\n",
    "#answer=Missing not at random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c2a34",
   "metadata": {},
   "source": [
    "# Missing investors\n",
    "Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.\n",
    "\n",
    "You just received a new version of the banking DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing inv_amount values.\n",
    "\n",
    "You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. The pandas, missingno and matplotlib.pyplot packages have been imported as pd, msno and plt respectively. The banking DataFrame is in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1da8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11997aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values(by = 'age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ea81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Question\n",
    "Now that you've isolated banking into investors and missing_investors, use the .describe() method on both of these DataFrames in the IPython shell to understand whether there are structural differences between them. What do you think is going on?\n",
    "\n",
    "Possible Answers\n",
    "\n",
    "- The data is missing completely at random and there are no drivers behind the missingness.\n",
    "\n",
    "- The inv_amount is missing only for young customers, since the average age in missing_investors is 22 and the maximum age is 25.\n",
    "\n",
    "- The inv_amount is missing only for old customers, since the average age in missing_investors is 42 and the maximum age is 59.\n",
    "\n",
    "\"\"\"\n",
    "#answer=The inv_amount is missing only for young customers, since the average age in missing_investors is 22 and the maximum age is 25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352400f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
